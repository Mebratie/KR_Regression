{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f0ee4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data (x1, x2, x3, x4, energy):\n",
      "0.1072399600573416, -0.6104307892807072, 0.15129249180732407, -0.1424946641942636, 0.2824606919596796\n",
      "0.5112196439839398, 0.4594951029940697, 0.12075093961111255, -0.17002469484863553, 0.3457336044742883\n",
      "-0.5647253234823525, 0.14345668871439532, -0.13514946929667093, 0.0002735230724775217, 0.22364632439261045\n",
      "-0.4956958705028317, -0.5228321821919234, -0.09116398624687282, 0.189160196365885, 0.2007521096728534\n",
      "0.20245999892288769, 0.30876385229616443, 0.18702110659842924, -0.13367248559802464, 0.0974294311787947\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "output_directory = r'../../../Data/Henon_chaotic_T5_40_50_60_70_80_90_100/Henon_chaotic_T5_50'\n",
    "output_director = r'../../../System/Henon_chaotic_T5_40_50_60_70_80_90_100/Henon_chaotic_T5_50'\n",
    "m = 5 # number of trajectory\n",
    "m1 = 25 # data point per trajectory divided by 2, which means the total data point is m*(2m1)=40\n",
    "m2 = 26 \n",
    "def generate_random_values():\n",
    "    a1 = -0.7\n",
    "    a2 = 0.7\n",
    "    a3 = -0.2\n",
    "    a4 = 0.2\n",
    "    x1 = np.random.uniform(a1, a2)\n",
    "    x2 = np.random.uniform(a1, a2)\n",
    "    x3 = np.random.uniform(a3, a4)\n",
    "    x4 = np.random.uniform(a3, a4)\n",
    "    return x1, x2, x3, x4\n",
    "def generate_random_values_based_on_c():\n",
    "    a1 = -0.7\n",
    "    a2 = 0.7\n",
    "    a3 = -0.2\n",
    "    a4 = 0.2\n",
    "    x1 = np.random.uniform(a1, a2)\n",
    "    x2 = np.random.uniform(a1, a2)\n",
    "    x3 = np.random.uniform(a3, a4)\n",
    "    x4 = np.random.uniform(a3, a4)\n",
    "    return x1, x2, x3, x4\n",
    "def compute_energy(x1, x2, x3, x4):\n",
    "    return 0.5 * (x3**2 + x4**2) + 0.5 * (x1**2 + x2**2) + x1**2 * x2 - (1/3) * x2**3\n",
    "def generate_data(initial_conditions):\n",
    "    def normalize(vector):\n",
    "        norm = np.linalg.norm(vector)\n",
    "        if norm == 0: \n",
    "            return vector\n",
    "        return vector / norm\n",
    "    def normalized_system(y, t):\n",
    "        x1, x2, x3, x4 = y\n",
    "        f = np.array([x3, x4, -x1 - 2*x1*x2, -x2 - x1**2 + x2**2])\n",
    "        normalized_f = normalize(f)\n",
    "        return normalized_f\n",
    "    t1 = np.linspace(0, 10, m1) # forward\n",
    "    t2 = np.linspace(0, 10, m2) # backward\n",
    "    forward_trajectories = []\n",
    "    backward_trajectories = []\n",
    "    initial_conditions_to_print = []\n",
    "    print(\"Initial data (x1, x2, x3, x4, energy):\")\n",
    "    for initial_condition in initial_conditions:\n",
    "        x1, x2, x3, x4 = initial_condition\n",
    "        energy = compute_energy(x1, x2, x3, x4)\n",
    "        print(f\"{x1}, {x2}, {x3}, {x4}, {energy}\")\n",
    "        forward_trajectory = odeint(normalized_system, initial_condition, t1)\n",
    "        backward_trajectory = odeint(normalized_system, initial_condition, -t2)\n",
    "        forward_trajectories.append(forward_trajectory[:m1])\n",
    "        backward_trajectories.append(backward_trajectory[:m2])\n",
    "    return forward_trajectories, backward_trajectories, t1[:m1], t2[:m2]\n",
    "def save_data(forward_trajectories, backward_trajectories, t1, t2):\n",
    "    num_variables = 4 \n",
    "    column_names = [f'x{i+1}' for i in range(num_variables)]\n",
    "    column_names.append('trajectory')    \n",
    "    with open(os.path.join(output_director, '50.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(column_names)\n",
    "        for r, (forward_data, backward_data) in enumerate(zip(forward_trajectories, backward_trajectories)):\n",
    "            for j in range(len(t1)):\n",
    "                forward_row = forward_data[j].tolist() + [r + 1]\n",
    "                writer.writerow(forward_row)\n",
    "            for j in range(1, len(t2)):  # Exclude initial conditions for backward trajectory\n",
    "                backward_row = backward_data[j].tolist() + [r + 1]\n",
    "                writer.writerow(backward_row)\n",
    "    output_directory1 = r'../../../results/Henon_chaotic_T5_40_50_60_70_80_90_100/Henon_chaotic_T5_50'\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, (forward_sol, backward_sol) in enumerate(zip(forward_trajectories, backward_trajectories)):\n",
    "        for j in range(forward_sol.shape[1]):\n",
    "            plt.plot(t1, forward_sol[:, j])\n",
    "            plt.plot(-t2, backward_sol[:, j])  # Exclude initial conditions for backward trajectory\n",
    "    plt.savefig(os.path.join(output_directory1, 'trajectory.png'))\n",
    "    plt.close()\n",
    "def split_data():\n",
    "    trajectories = {}\n",
    "    column_names = None    \n",
    "    with open(os.path.join(output_directory, 'trainingp_data50.csv'), 'r') as trainfile:\n",
    "        reader = csv.DictReader(trainfile)\n",
    "        column_names = reader.fieldnames\n",
    "        for row in reader:\n",
    "            trajectory = float(row['trajectory'])\n",
    "            if trajectory not in trajectories:\n",
    "                trajectories[trajectory] = []\n",
    "            trajectory_data = {key: float(value) for key, value in row.items()}\n",
    "            trajectories[trajectory].append(trajectory_data)   \n",
    "    for traj_points in trajectories.values():\n",
    "        random.shuffle(traj_points)   \n",
    "    num_points_per_file = len(next(iter(trajectories.values()))) // 5  # divide into five splits (n stratify)    \n",
    "    for i in range(5):  # Five-fold cross-validation\n",
    "        output_filename = f'B50{i+1}.csv'\n",
    "        with open(os.path.join(output_directory, output_filename), 'w', newline='') as output_file:\n",
    "            writer = csv.DictWriter(output_file, fieldnames=column_names)\n",
    "            writer.writeheader()\n",
    "            for trajectory, points in trajectories.items():\n",
    "                for point in points[i * num_points_per_file: (i + 1) * num_points_per_file]:\n",
    "                    writer.writerow(point) \n",
    "if __name__ == \"__main__\":\n",
    "    x1, x2, x3, x4 = generate_random_values()\n",
    "    initial_conditions = [generate_random_values_based_on_c() for _ in range(m)]\n",
    "    forward_trajectories, backward_trajectories, t1, t2 = generate_data(initial_conditions)\n",
    "    save_data(forward_trajectories, backward_trajectories, t1, t2)\n",
    "\n",
    "    data = np.genfromtxt(os.path.join(output_director, '50.csv'), delimiter=',', names=True)\n",
    "    training_data = []\n",
    "    holdout_data = []\n",
    "    for r in range(1, 6):  # this represents the number of initial data is 5. i.e., (1,6) means 5 initial data\n",
    "        trajectory_subset = data[data['trajectory'] == r]\n",
    "        train_set, holdout_set = train_test_split(trajectory_subset, test_size=0.2, random_state=42)\n",
    "        training_data.extend(train_set)\n",
    "        holdout_data.extend(holdout_set)\n",
    "    column_names = data.dtype.names\n",
    "    with open(os.path.join(output_directory, 'trainingp_data50.csv'), 'w', newline='') as trainfile:\n",
    "        writer = csv.writer(trainfile)\n",
    "        writer.writerow(column_names)\n",
    "        for row in training_data:\n",
    "            writer.writerow([row[col] for col in column_names])\n",
    "    with open(os.path.join(output_directory, 'holdoutp_data50.csv'), 'w', newline='') as holdfile:\n",
    "        writer = csv.writer(holdfile)\n",
    "        writer.writerow(column_names)\n",
    "        for row in holdout_data:\n",
    "            writer.writerow([row[col] for col in column_names])\n",
    "    split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "294a4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "input_directory = r'../../../System/Henon_chaotic_T5_40_50_60_70_80_90_100/Henon_chaotic_T5_50'\n",
    "output_directory = r'../../../Data/Henon_chaotic_T5_40_50_60_70_80_90_100/Henon_chaotic_T5_50/Traj_data'\n",
    "df14 = pd.read_csv(os.path.join(input_directory, '50.csv')) # 50.csv, saved names of the data\n",
    "Br = pd.read_csv('../../../System/Henon_chaotic_T5_40_50_60_70_80_90_100/Henon_chaotic_T5_50/50.csv')\n",
    "tr = Br.groupby('trajectory').size()\n",
    "re1 = int(round(tr.mean()))\n",
    "rows_per_file = re1 \n",
    "num_files = len(df14) // rows_per_file\n",
    "data_chunks = [df14.iloc[i * rows_per_file:(i + 1) * rows_per_file].iloc[:, :-1] for i in range(num_files)]\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "for i, chunk in enumerate(data_chunks):\n",
    "    chunk.to_csv(os.path.join(output_directory, f'tr_{i + 1}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ebe87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd66f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
