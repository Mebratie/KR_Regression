{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f0ee4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data (x1, x2, x3, x4):\n",
      "(-0.20412431878371318, 0.34929267573552036, -0.0022128020906907464, -0.009577699553046506)\n",
      "(-0.2063482957198448, 0.05375585756482981, -0.08700874266196908, 0.042837824497913374)\n",
      "(-0.46709522504329526, -0.10950325595660804, 0.06990107602383358, -0.013732942400671466)\n",
      "(0.1851453500966913, 0.16692338916958827, 0.06821467586413543, 0.06381186161139205)\n",
      "(0.3187363684650544, -0.12038859981257877, -0.08708893898052537, 0.010324414779968594)\n",
      "Energy for Trajectory 1: 0.08223305288392527\n",
      "Energy for Trajectory 2: 0.02967456218933511\n",
      "Energy for Trajectory 3: 0.09416828729024075\n",
      "Energy for Trajectory 4: 0.03960520147045723\n",
      "Energy for Trajectory 5: 0.05023970643738751\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import sympy as sp\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sympy import symbols, Function, diff\n",
    "output_directory = r'../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_50'\n",
    "def generate_random_values():\n",
    "    a1 = -0.5\n",
    "    a2 = 0.5\n",
    "    a3 = -0.1\n",
    "    a4 = 0.1\n",
    "    x1 = np.random.uniform(a1, a2)\n",
    "    x2 = np.random.uniform(a1, a2)\n",
    "    x3 = np.random.uniform(a3, a4)\n",
    "    x4 = np.random.uniform(a3, a4)\n",
    "    return x1, x2, x3, x4\n",
    "def generate_random_values_based_on_c():\n",
    "    a1 = -0.5\n",
    "    a2 = 0.5\n",
    "    a3 = -0.1\n",
    "    a4 = 0.1\n",
    "    x1 = np.random.uniform(a1, a2)\n",
    "    x2 = np.random.uniform(a1, a2)\n",
    "    x3 = np.random.uniform(a3, a4)\n",
    "    x4 = np.random.uniform(a3, a4)\n",
    "    return x1, x2, x3, x4\n",
    "def generate_data(initial_conditions):\n",
    "    def normalize(vector):\n",
    "        norm = np.linalg.norm(vector)\n",
    "        if norm == 0: \n",
    "            return vector\n",
    "        return vector / norm\n",
    "    def normalized_system(y, t):\n",
    "        x1, x2, x3, x4 = y\n",
    "        f = np.array([x3, x4, -x1 - 2*x1*x2, -x2 - x1**2 + x2**2])\n",
    "        normalized_f = normalize(f)\n",
    "        return normalized_f\n",
    "    def compute_energy(x1, x2, x3, x4):\n",
    "        return 0.5 * (x3**2 + x4**2) + 0.5 * (x1**2 + x2**2) + x1**2 * x2 - (1/3) * x2**3\n",
    "    num_trajectories = 5\n",
    "    t = np.linspace(0, 10, 50) # 40 data points per trajectory\n",
    "    all_trajectory_data = []\n",
    "    initial_conditions_to_print = []\n",
    "    print(\"Initial data (x1, x2, x3, x4):\")\n",
    "    for i, initial_condition in enumerate(initial_conditions):\n",
    "        print(f\"({initial_condition[0]}, {initial_condition[1]}, {initial_condition[2]}, {initial_condition[3]})\")\n",
    "        sol = odeint(normalized_system, initial_condition, t)\n",
    "        all_trajectory_data.append(sol)\n",
    "        final_state = sol[-1, :]\n",
    "        E = compute_energy(*final_state)\n",
    "        initial_conditions_to_print.append((initial_condition, E))\n",
    "    num_variables = 4 # Adjust number of variables that we need for the regression accordingly\n",
    "    column_names = [f'x{i+1}' for i in range(num_variables)]\n",
    "    column_names.append('trajectory')\n",
    "    with open('50.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(column_names)\n",
    "        for r, data in enumerate(all_trajectory_data):\n",
    "            for j in range(len(t)):\n",
    "                row = data[j].tolist() + [r + 1]\n",
    "                writer.writerow(row) \n",
    "    for r, (initial_conditions, E) in enumerate(initial_conditions_to_print):\n",
    "        print(f\"Energy for Trajectory {r+1}: {E}\")\n",
    "    output_directory1 = r'../../../results/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_50'\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, sol in enumerate(all_trajectory_data):\n",
    "        for j in range(sol.shape[1]):\n",
    "            plt.plot(t, sol[:, j])\n",
    "    plt.savefig(os.path.join(output_directory1, 'trajectory.png'))\n",
    "    plt.close()\n",
    "def split_data():\n",
    "    trajectories = {}\n",
    "    column_names = None\n",
    "    with open('../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_50/trainingp_data50.csv', 'r') as trainfile:\n",
    "        reader = csv.DictReader(trainfile)\n",
    "        column_names = reader.fieldnames\n",
    "        for row in reader:\n",
    "            trajectory = float(row['trajectory'])\n",
    "            if trajectory not in trajectories:\n",
    "                trajectories[trajectory] = []\n",
    "            trajectory_data = {key: float(value) for key, value in row.items()}\n",
    "            trajectories[trajectory].append(trajectory_data)\n",
    "    for traj_points in trajectories.values():\n",
    "        random.shuffle(traj_points)\n",
    "    num_points_per_file = len(next(iter(trajectories.values()))) // 5  # divide into five splits (n stratify)\n",
    "    for i in range(5):  # Five-fold cross-validation\n",
    "        output_filename = f'B50{i+1}.csv'\n",
    "        with open(os.path.join(output_directory, output_filename), 'w', newline='') as output_file:\n",
    "            writer = csv.DictWriter(output_file, fieldnames=column_names)\n",
    "            writer.writeheader()\n",
    "            for trajectory, points in trajectories.items():\n",
    "                for point in points[i * num_points_per_file: (i + 1) * num_points_per_file]:\n",
    "                    writer.writerow(point)\n",
    "if __name__ == \"__main__\":\n",
    "    x1, x2, x3, x4 = generate_random_values()\n",
    "    initial_conditions = [generate_random_values_based_on_c() for _ in range(5)]  # number of trajectories\n",
    "    generate_data(initial_conditions)\n",
    "    data = np.genfromtxt('50.csv', delimiter=',', names=True)\n",
    "    training_data = []\n",
    "    holdout_data = []\n",
    "    for r in range(1, 6):  # this represents the number of initial data is 5. i.e., (1,6) means 5 initial data\n",
    "        trajectory_subset = data[data['trajectory'] == r]\n",
    "        train_set, holdout_set = train_test_split(trajectory_subset, test_size=0.2, random_state=42)\n",
    "        training_data.extend(train_set)\n",
    "        holdout_data.extend(holdout_set)\n",
    "    column_names = data.dtype.names\n",
    "    with open(os.path.join(output_directory, 'trainingp_data50.csv'), 'w', newline='') as trainfile:\n",
    "        writer = csv.writer(trainfile)\n",
    "        writer.writerow(column_names)\n",
    "        for row in training_data:\n",
    "            writer.writerow([row[col] for col in column_names])\n",
    "    with open(os.path.join(output_directory, 'holdoutp_data50.csv'), 'w', newline='') as holdfile:\n",
    "        writer = csv.writer(holdfile)\n",
    "        writer.writerow(column_names)\n",
    "        for row in holdout_data:\n",
    "            writer.writerow([row[col] for col in column_names])\n",
    "    split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "294a4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = r'../../../System/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_50'\n",
    "output_directory = r'../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_50/Traj_data'\n",
    "df14 = pd.read_csv(os.path.join(input_directory, '50.csv')) # 50.csv, saved names of the data\n",
    "Br = pd.read_csv('../../../System/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_50/50.csv')\n",
    "tr = Br.groupby('trajectory').size()\n",
    "re1 = int(round(tr.mean()))\n",
    "rows_per_file = re1 \n",
    "num_files = len(df14) // rows_per_file\n",
    "data_chunks = [df14.iloc[i * rows_per_file:(i + 1) * rows_per_file].iloc[:, :-1] for i in range(num_files)]\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "for i, chunk in enumerate(data_chunks):\n",
    "    chunk.to_csv(os.path.join(output_directory, f'tr_{i + 1}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "output_directory = r'../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_50'\n",
    "output_director = r'../../../System/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_50'\n",
    "m = 5 # number of trajectory\n",
    "m1 = 25 # data point from the forward\n",
    "m2 = 26 # data point from the backward\n",
    "def generate_random_values():\n",
    "    a1 = -0.7\n",
    "    a2 = 0.7\n",
    "    a3 = -0.2\n",
    "    a4 = 0.2\n",
    "    x1 = np.random.uniform(a1, a2)\n",
    "    x2 = np.random.uniform(a1, a2)\n",
    "    x3 = np.random.uniform(a3, a4)\n",
    "    x4 = np.random.uniform(a3, a4)\n",
    "    return x1, x2, x3, x4\n",
    "def generate_random_values_based_on_c():\n",
    "    a1 = -0.7\n",
    "    a2 = 0.7\n",
    "    a3 = -0.2\n",
    "    a4 = 0.2\n",
    "    x1 = np.random.uniform(a1, a2)\n",
    "    x2 = np.random.uniform(a1, a2)\n",
    "    x3 = np.random.uniform(a3, a4)\n",
    "    x4 = np.random.uniform(a3, a4)\n",
    "    return x1, x2, x3, x4\n",
    "def compute_energy(x1, x2, x3, x4):\n",
    "    return 0.5 * (x3**2 + x4**2) + 0.5 * (x1**2 + x2**2) + x1**2 * x2 - (1/3) * x2**3\n",
    "def generate_data(initial_conditions):\n",
    "    def normalize(vector):\n",
    "        norm = np.linalg.norm(vector)\n",
    "        if norm == 0: \n",
    "            return vector\n",
    "        return vector / norm\n",
    "    def normalized_system(y, t):\n",
    "        x1, x2, x3, x4 = y\n",
    "        f = np.array([x3, x4, -x1 - 2*x1*x2, -x2 - x1**2 + x2**2])\n",
    "        normalized_f = normalize(f)\n",
    "        return normalized_f\n",
    "    t1 = np.linspace(0, 10, 50)\n",
    "    t2 = np.linspace(0, 10, 50)\n",
    "    forward_trajectories = []\n",
    "    backward_trajectories = []\n",
    "    initial_conditions_to_print = []\n",
    "    print(\"Initial data (x1, x2, x3, x4, energy):\")\n",
    "    for initial_condition in initial_conditions:\n",
    "        x1, x2, x3, x4 = initial_condition\n",
    "        energy = compute_energy(x1, x2, x3, x4)\n",
    "        print(f\"{x1}, {x2}, {x3}, {x4}, {energy}\")\n",
    "        forward_trajectory = odeint(normalized_system, initial_condition, t1)\n",
    "        backward_trajectory = odeint(normalized_system, initial_condition, -t2)\n",
    "        forward_trajectories.append(forward_trajectory[:m1])\n",
    "        backward_trajectories.append(backward_trajectory[:m2])\n",
    "    return forward_trajectories, backward_trajectories, t1[:m1], t2[:m2]\n",
    "def save_data(forward_trajectories, backward_trajectories, t1, t2):\n",
    "    num_variables = 4 \n",
    "    column_names = [f'x{i+1}' for i in range(num_variables)]\n",
    "    column_names.append('trajectory')    \n",
    "    with open(os.path.join(output_director, '50.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(column_names)\n",
    "        for r, (forward_data, backward_data) in enumerate(zip(forward_trajectories, backward_trajectories)):\n",
    "            for j in range(len(t1)):\n",
    "                forward_row = forward_data[j].tolist() + [r + 1]\n",
    "                writer.writerow(forward_row)\n",
    "            for j in range(1, len(t2)):  # Exclude initial conditions for backward trajectory\n",
    "                backward_row = backward_data[j].tolist() + [r + 1]\n",
    "                writer.writerow(backward_row)\n",
    "    output_directory1 = r'../../../results/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_50'\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, (forward_sol, backward_sol) in enumerate(zip(forward_trajectories, backward_trajectories)):\n",
    "        for j in range(forward_sol.shape[1]):\n",
    "            plt.plot(t1, forward_sol[:, j])\n",
    "            plt.plot(-t2, backward_sol[:, j])  # Exclude initial conditions for backward trajectory\n",
    "    plt.savefig(os.path.join(output_directory1, 'trajectory.png'))\n",
    "    plt.close()\n",
    "def split_data():\n",
    "    trajectories = {}\n",
    "    column_names = None    \n",
    "    with open(os.path.join(output_directory, 'trainingp_data50.csv'), 'r') as trainfile:\n",
    "        reader = csv.DictReader(trainfile)\n",
    "        column_names = reader.fieldnames\n",
    "        for row in reader:\n",
    "            trajectory = float(row['trajectory'])\n",
    "            if trajectory not in trajectories:\n",
    "                trajectories[trajectory] = []\n",
    "            trajectory_data = {key: float(value) for key, value in row.items()}\n",
    "            trajectories[trajectory].append(trajectory_data)   \n",
    "    for traj_points in trajectories.values():\n",
    "        random.shuffle(traj_points)   \n",
    "    num_points_per_file = len(next(iter(trajectories.values()))) // 5  # divide into five splits (n stratify)    \n",
    "    for i in range(5):  # Five-fold cross-validation\n",
    "        output_filename = f'B50{i+1}.csv'\n",
    "        with open(os.path.join(output_directory, output_filename), 'w', newline='') as output_file:\n",
    "            writer = csv.DictWriter(output_file, fieldnames=column_names)\n",
    "            writer.writeheader()\n",
    "            for trajectory, points in trajectories.items():\n",
    "                for point in points[i * num_points_per_file: (i + 1) * num_points_per_file]:\n",
    "                    writer.writerow(point) \n",
    "if __name__ == \"__main__\":\n",
    "    x1, x2, x3, x4 = generate_random_values()\n",
    "    initial_conditions = [generate_random_values_based_on_c() for _ in range(m)]\n",
    "    forward_trajectories, backward_trajectories, t1, t2 = generate_data(initial_conditions)\n",
    "    save_data(forward_trajectories, backward_trajectories, t1, t2)\n",
    "\n",
    "    data = np.genfromtxt(os.path.join(output_director, '50.csv'), delimiter=',', names=True)\n",
    "    training_data = []\n",
    "    holdout_data = []\n",
    "    for r in range(1, 6):  # this represents the number of initial data is 5. i.e., (1,6) means 5 initial data\n",
    "        trajectory_subset = data[data['trajectory'] == r]\n",
    "        train_set, holdout_set = train_test_split(trajectory_subset, test_size=0.2, random_state=42)\n",
    "        training_data.extend(train_set)\n",
    "        holdout_data.extend(holdout_set)\n",
    "    column_names = data.dtype.names\n",
    "    with open(os.path.join(output_directory, 'trainingp_data50.csv'), 'w', newline='') as trainfile:\n",
    "        writer = csv.writer(trainfile)\n",
    "        writer.writerow(column_names)\n",
    "        for row in training_data:\n",
    "            writer.writerow([row[col] for col in column_names])\n",
    "    with open(os.path.join(output_directory, 'holdoutp_data50.csv'), 'w', newline='') as holdfile:\n",
    "        writer = csv.writer(holdfile)\n",
    "        writer.writerow(column_names)\n",
    "        for row in holdout_data:\n",
    "            writer.writerow([row[col] for col in column_names])\n",
    "    split_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
