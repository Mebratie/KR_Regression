{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abfa4fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 1e-07}\n",
      "Best RMSE: 1.2875193383997586e-17\n",
      "\n",
      "\n",
      "Candidate Conservation Law:\n",
      "                      3                        2                            2 \n",
      "9.35647208644994e-8⋅x₁  + 0.0100544965859799⋅x₁ ⋅x₂ + 1.81783861864276e-7⋅x₁ ⋅\n",
      "\n",
      "                          2                            2                      \n",
      "x₃ - 1.8796218007676e-9⋅x₁ ⋅x₄ + 0.00502727642060443⋅x₁  + 5.1409054928449e-8⋅\n",
      "\n",
      "     2                                                                        \n",
      "x₁⋅x₂  + 5.68658355332759e-9⋅x₁⋅x₂⋅x₃ + 1.82451909983168e-7⋅x₁⋅x₂⋅x₄ + 1.65587\n",
      "\n",
      "                                              2                               \n",
      "007112727e-8⋅x₁⋅x₂ + 1.10702047571387e-7⋅x₁⋅x₃  + 1.78906790477678e-7⋅x₁⋅x₃⋅x₄\n",
      "\n",
      "                                                         2                    \n",
      " + 6.69243406786063e-10⋅x₁⋅x₃ + 2.22066017379225e-8⋅x₁⋅x₄  + 8.71497757759434e\n",
      "\n",
      "                                                         3                    \n",
      "-8⋅x₁⋅x₄ - 1.1734603924847e-8⋅x₁ - 0.00335148356306731⋅x₂  + 2.38681120844791e\n",
      "\n",
      "     2                            2                            2              \n",
      "-7⋅x₂ ⋅x₃ - 4.78447435093904e-8⋅x₂ ⋅x₄ + 0.00502730597928164⋅x₂  - 2.536182365\n",
      "\n",
      "             2                                                                \n",
      "2591e-7⋅x₂⋅x₃  + 4.41246110672753e-8⋅x₂⋅x₃⋅x₄ - 9.30704700911889e-8⋅x₂⋅x₃ + 6.\n",
      "\n",
      "                       2                                                      \n",
      "56799490119102e-8⋅x₂⋅x₄  + 7.38752224324624e-10⋅x₂⋅x₄ - 2.21011931585829e-9⋅x₂\n",
      "\n",
      "                         3                         2                          \n",
      " + 2.32967860800234e-7⋅x₃  + 3.45116822309758e-8⋅x₃ ⋅x₄ + 0.00502731706821901⋅\n",
      "\n",
      "  2                            2                                              \n",
      "x₃  + 3.57896136793227e-7⋅x₃⋅x₄  + 1.67725370866373e-8⋅x₃⋅x₄ - 2.7742047022452\n",
      "\n",
      "                                3                         2                   \n",
      "6e-8⋅x₃ - 7.02108648132044e-8⋅x₄  + 0.00502731573245182⋅x₄  + 7.69804035620492\n",
      "\n",
      "                          \n",
      "e-9⋅x₄ + 0.999466228406152\n",
      "\n",
      "\n",
      "Final Candidate CL:\n",
      "                     2                            2                         3 \n",
      "0.0100544965859799⋅x₁ ⋅x₂ + 0.00502727642060443⋅x₁  - 0.00335148356306731⋅x₂  \n",
      "\n",
      "                        2                         2                         2 \n",
      "+ 0.00502730597928164⋅x₂  + 0.00502731706821901⋅x₃  + 0.00502731573245182⋅x₄  \n",
      "\n",
      "                   \n",
      "+ 0.999466228406152\n",
      "\n",
      "\n",
      "Simplified Candidate CL:\n",
      "      2                          2                       3                    \n",
      "1.0⋅x₁ ⋅x₂ + 0.500002797515942⋅x₁  - 0.333331811732937⋅x₂  + 0.500005737362502\n",
      "\n",
      "   2                       2                       2\n",
      "⋅x₂  + 0.500006840245901⋅x₃  + 0.500006707393184⋅x₄ \n",
      "\n",
      "\n",
      "Generalisation Error (RMSE): 2.3854889455357086e-09\n",
      "\n",
      " Relative deviation: 1.1514593186640315e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "import csv\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sympy import symbols, simplify, lambdify, Function, diff, Mul\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "%matplotlib inline\n",
    "dm = pd.read_csv(\"../../../System/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/50.csv\")\n",
    "m = dm['trajectory'].nunique()\n",
    "x_columns = [col for col in dm.columns if col.startswith('x')]\n",
    "num_x_variables = len(x_columns)\n",
    "num_y_variables = m\n",
    "def compute_kernel_matrix(X, c, d):\n",
    "    n = X.shape[0]\n",
    "    K = (c + np.dot(X, X.T)) ** d\n",
    "    return K\n",
    "def solve_for_lambda(data, c, d, lambda_value, m):\n",
    "    B501_data = pd.read_csv('../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B501.csv')\n",
    "    traj_len = B501_data.groupby('trajectory').size()\n",
    "    rep = int(round(traj_len.mean()))\n",
    "    K = compute_kernel_matrix(data, c, d)\n",
    "    I = np.eye(K.shape[0])\n",
    "    K_with_I = K + lambda_value * I\n",
    "    K_with_I_inv = np.linalg.inv(K_with_I)\n",
    "    x_symbols = [sp.symbols(f'x{i}') for i in range(1, num_x_variables + 1)]\n",
    "    y_symbols = [sp.symbols(f'y{i}') for i in range(num_y_variables)]\n",
    "    y0 = sp.symbols('y0')\n",
    "    y_pattern = [sp.symbols(f'y{i}') for i in range(m)]\n",
    "    y_repeated = np.repeat(y_pattern, rep, axis=0)\n",
    "    y = sp.Matrix([sp.symbols(f'y{i}') for i in range(m)])\n",
    "    M_matrix = sp.Matrix(K_with_I_inv)\n",
    "    n = K.shape[0]\n",
    "    M = sp.zeros(n, m)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            start_idx = j * rep\n",
    "            end_idx = (j + 1) * rep\n",
    "            M[i, j] = sp.Add(*M_matrix[i, start_idx:end_idx])\n",
    "    M_transpose = M.transpose()\n",
    "    A = M_transpose @ M\n",
    "    C = y.T\n",
    "    D = C @ A @ y\n",
    "    D_expanded = sp.expand(D.subs(y0, 1))\n",
    "    W1 = sp.Matrix([sp.diff(D_expanded, var) for var in y])\n",
    "    B = W1.subs(y0, 1)\n",
    "    system_of_equations = []\n",
    "    variables = [sp.symbols(f'y{i}') for i in range(1, m)]\n",
    "    for i in range(len(y_pattern)-1):\n",
    "        equation = sp.Eq(B[i+1], 0)\n",
    "        system_of_equations.append(equation)\n",
    "    solution = sp.solve(system_of_equations, variables)\n",
    "    solution_with_y0 = {**{'y0': 1}, **solution}\n",
    "    return {'W1': W1, 'solution': solution_with_y0}\n",
    "def solve_for_lambda_A2(data_A2, c, d, lambda_value, m):\n",
    "    B501_data = pd.read_csv('../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B501.csv')\n",
    "    traj_len = B501_data.groupby('trajectory').size()\n",
    "    rep = int(round(traj_len.mean()))\n",
    "    K = compute_kernel_matrix(data_A2, c, d)\n",
    "    I = np.eye(K.shape[0])\n",
    "    K_with_I = K + lambda_value * I\n",
    "    K_with_I_inv = np.linalg.inv(K_with_I)\n",
    "    x_symbols = [sp.symbols(f'x{i}') for i in range(1, num_x_variables + 1)]\n",
    "    y_symbols = [sp.symbols(f'y{i}') for i in range(num_y_variables)]\n",
    "    y0 = sp.symbols('y0')\n",
    "    substitutions = {old: new for old, new in zip([f'x{i}' for i in range(1, num_x_variables + 1)], x_symbols)}\n",
    "    substitutions.update({old: new for old, new in zip([f'y{i}' for i in range(num_y_variables)], y_symbols)})\n",
    "    K_with_I_inv_A2 = sp.Matrix(K_with_I_inv).subs(substitutions)\n",
    "    y_pattern = [sp.symbols(f'y{i}') for i in range(m)]\n",
    "    y_repeated = np.repeat(y_pattern, rep, axis=0)\n",
    "    y = sp.Matrix([sp.symbols(f'y{i}') for i in range(m)])\n",
    "    M_matrix = sp.Matrix(K_with_I_inv_A2)\n",
    "    n = K.shape[0]\n",
    "    M = sp.zeros(n, m)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            start_idx = j * rep\n",
    "            end_idx = (j + 1) * rep\n",
    "            M[i, j] = sp.Add(*M_matrix[i, start_idx:end_idx])\n",
    "    M_transpose = M.transpose()\n",
    "    A = M_transpose @ M\n",
    "    C = y.T\n",
    "    D = C @ A @ y\n",
    "    D_expanded = sp.expand(D.subs(y0, 1))\n",
    "    W2 = sp.Matrix([sp.diff(D_expanded, var) for var in y])\n",
    "    B = W2.subs(y0, 1)\n",
    "    system_of_equations = []\n",
    "    variables = [sp.symbols(f'y{i}') for i in range(1, m)]\n",
    "    for i in range(len(y_pattern)-1):\n",
    "        equation = sp.Eq(B[i+1], 0)\n",
    "        system_of_equations.append(equation)\n",
    "    solution = sp.solve(system_of_equations, variables)\n",
    "    solution_with_y0 = {**{'y0': 1}, **solution}\n",
    "    return {'W2': W2, 'solution': solution_with_y0}\n",
    "def solve_for_lambda_A3(data_A3, c, d, lambda_value, m):\n",
    "    B501_data = pd.read_csv('../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B501.csv')\n",
    "    traj_len = B501_data.groupby('trajectory').size()\n",
    "    rep = int(round(traj_len.mean()))\n",
    "    K = compute_kernel_matrix(data_A2, c, d)\n",
    "    I = np.eye(K.shape[0])\n",
    "    K_with_I = K + lambda_value * I\n",
    "    K_with_I_inv = np.linalg.inv(K_with_I)\n",
    "    x_symbols = [sp.symbols(f'x{i}') for i in range(1, num_x_variables + 1)]\n",
    "    y_symbols = [sp.symbols(f'y{i}') for i in range(num_y_variables)]\n",
    "    y0 = sp.symbols('y0')\n",
    "    substitutions = {old: new for old, new in zip([f'x{i}' for i in range(1, num_x_variables + 1)], x_symbols)}\n",
    "    substitutions.update({old: new for old, new in zip([f'y{i}' for i in range(num_y_variables)], y_symbols)})\n",
    "    K_with_I_inv_A3 = sp.Matrix(K_with_I_inv).subs(substitutions)\n",
    "    y_pattern = [sp.symbols(f'y{i}') for i in range(m)]\n",
    "    y_repeated = np.repeat(y_pattern, rep, axis=0)\n",
    "    y = sp.Matrix([sp.symbols(f'y{i}') for i in range(m)])\n",
    "    M_matrix = sp.Matrix(K_with_I_inv_A3)\n",
    "    n = K.shape[0]\n",
    "    M = sp.zeros(n, m)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            start_idx = j * rep\n",
    "            end_idx = (j + 1) * rep\n",
    "            M[i, j] = sp.Add(*M_matrix[i, start_idx:end_idx])\n",
    "    M_transpose = M.transpose()\n",
    "    A = M_transpose @ M\n",
    "    C = y.T\n",
    "    D = C @ A @ y\n",
    "    D_expanded = sp.expand(D.subs(y0, 1))\n",
    "    W3 = sp.Matrix([sp.diff(D_expanded, var) for var in y])\n",
    "    B = W3.subs(y0, 1)\n",
    "    system_of_equations = []\n",
    "    variables = [sp.symbols(f'y{i}') for i in range(1, m)]\n",
    "    for i in range(len(y_pattern)-1):\n",
    "        equation = sp.Eq(B[i+1], 0)\n",
    "        system_of_equations.append(equation)\n",
    "    solution = sp.solve(system_of_equations, variables)\n",
    "    solution_with_y0 = {**{'y0': 1}, **solution}\n",
    "    return {'W3': W3, 'solution': solution_with_y0}\n",
    "def solve_for_lambda_A4(data_A4, c, d, lambda_value, m):\n",
    "    B501_data = pd.read_csv('../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B501.csv')\n",
    "    traj_len = B501_data.groupby('trajectory').size()\n",
    "    rep = int(round(traj_len.mean()))\n",
    "    K = compute_kernel_matrix(data_A2, c, d)\n",
    "    I = np.eye(K.shape[0])\n",
    "    K_with_I = K + lambda_value * I\n",
    "    K_with_I_inv = np.linalg.inv(K_with_I)\n",
    "    x_symbols = [sp.symbols(f'x{i}') for i in range(1, num_x_variables + 1)]\n",
    "    y_symbols = [sp.symbols(f'y{i}') for i in range(num_y_variables)]\n",
    "    y0 = sp.symbols('y0')\n",
    "    substitutions = {old: new for old, new in zip([f'x{i}' for i in range(1, num_x_variables + 1)], x_symbols)}\n",
    "    substitutions.update({old: new for old, new in zip([f'y{i}' for i in range(num_y_variables)], y_symbols)})\n",
    "    K_with_I_inv_A4 = sp.Matrix(K_with_I_inv).subs(substitutions)\n",
    "    y_pattern = [sp.symbols(f'y{i}') for i in range(m)]\n",
    "    y_repeated = np.repeat(y_pattern, rep, axis=0)\n",
    "    y = sp.Matrix([sp.symbols(f'y{i}') for i in range(m)])\n",
    "    M_matrix = sp.Matrix(K_with_I_inv_A4)\n",
    "    n = K.shape[0]\n",
    "    M = sp.zeros(n, m)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            start_idx = j * rep\n",
    "            end_idx = (j + 1) * rep\n",
    "            M[i, j] = sp.Add(*M_matrix[i, start_idx:end_idx])\n",
    "    M_transpose = M.transpose()\n",
    "    A = M_transpose @ M\n",
    "    C = y.T\n",
    "    D = C @ A @ y\n",
    "    D_expanded = sp.expand(D.subs(y0, 1))\n",
    "    W4 = sp.Matrix([sp.diff(D_expanded, var) for var in y])\n",
    "    B = W4.subs(y0, 1)\n",
    "    system_of_equations = []\n",
    "    variables = [sp.symbols(f'y{i}') for i in range(1, m)]\n",
    "    for i in range(len(y_pattern)-1):\n",
    "        equation = sp.Eq(B[i+1], 0)\n",
    "        system_of_equations.append(equation)\n",
    "    solution = sp.solve(system_of_equations, variables)\n",
    "    solution_with_y0 = {**{'y0': 1}, **solution}\n",
    "    return {'W4': W4, 'solution': solution_with_y0}\n",
    "def solve_for_lambda_A5(data_A5, c, d, lambda_value, m):\n",
    "    B501_data = pd.read_csv('../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B501.csv')\n",
    "    traj_len = B501_data.groupby('trajectory').size()\n",
    "    rep = int(round(traj_len.mean()))\n",
    "    K = compute_kernel_matrix(data_A5, c, d)\n",
    "    I = np.eye(K.shape[0])\n",
    "    K_with_I = K + lambda_value * I\n",
    "    K_with_I_inv = np.linalg.inv(K_with_I)\n",
    "    x_symbols = [sp.symbols(f'x{i}') for i in range(1, num_x_variables + 1)]\n",
    "    y_symbols = [sp.symbols(f'y{i}') for i in range(num_y_variables)]\n",
    "    y0 = sp.symbols('y0')\n",
    "    substitutions = {old: new for old, new in zip([f'x{i}' for i in range(1, num_x_variables + 1)], x_symbols)}\n",
    "    substitutions.update({old: new for old, new in zip([f'y{i}' for i in range(num_y_variables)], y_symbols)})\n",
    "    K_with_I_inv_A4 = sp.Matrix(K_with_I_inv).subs(substitutions)\n",
    "    y_pattern = [sp.symbols(f'y{i}') for i in range(m)]\n",
    "    y_repeated = np.repeat(y_pattern, rep, axis=0)\n",
    "    y = sp.Matrix([sp.symbols(f'y{i}') for i in range(m)])\n",
    "    M_matrix = sp.Matrix(K_with_I_inv_A4)\n",
    "    n = K.shape[0]\n",
    "    M = sp.zeros(n, m)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            start_idx = j * rep\n",
    "            end_idx = (j + 1) * rep\n",
    "            M[i, j] = sp.Add(*M_matrix[i, start_idx:end_idx])\n",
    "    M_transpose = M.transpose()\n",
    "    A = M_transpose @ M\n",
    "    C = y.T\n",
    "    D = C @ A @ y\n",
    "    D_expanded = sp.expand(D.subs(y0, 1))\n",
    "    W5 = sp.Matrix([sp.diff(D_expanded, var) for var in y])\n",
    "    B = W5.subs(y0, 1)\n",
    "    system_of_equations = []\n",
    "    variables = [sp.symbols(f'y{i}') for i in range(1, m)]\n",
    "    for i in range(len(y_pattern)-1):\n",
    "        equation = sp.Eq(B[i+1], 0)\n",
    "        system_of_equations.append(equation)\n",
    "    solution = sp.solve(system_of_equations, variables)\n",
    "    solution_with_y0 = {**{'y0': 1}, **solution}\n",
    "    return {'W5': W5, 'solution': solution_with_y0}\n",
    "def add_matrices(mat1, mat2, mat3, mat4, mat5):\n",
    "    return mat1 + mat2 + mat3 + mat4 + mat5\n",
    "data = np.loadtxt('../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B501.csv', delimiter=',', skiprows=1, usecols=(0, 1, 2, 3))\n",
    "data_A2 = np.loadtxt('../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B502.csv', delimiter=',', skiprows=1, usecols=(0, 1, 2, 3))\n",
    "data_A3 = np.loadtxt('../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B503.csv', delimiter=',', skiprows=1, usecols=(0, 1, 2, 3))\n",
    "data_A4 = np.loadtxt('../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B504.csv', delimiter=',', skiprows=1, usecols=(0, 1, 2, 3))\n",
    "data_A5 = np.loadtxt('../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B505.csv', delimiter=',', skiprows=1, usecols=(0, 1, 2, 3))\n",
    "lamda = 8 # order of magnitude for lambda value\n",
    "lambda_values_A1 = [10**(-i) for i in range(lamda)]\n",
    "lambda_values_A2 = [10**(-i) for i in range(lamda)]\n",
    "lambda_values_A3 = [10**(-i) for i in range(lamda)]\n",
    "lambda_values_A4 = [10**(-i) for i in range(lamda)]\n",
    "lambda_values_A5 = [10**(-i) for i in range(lamda)]\n",
    "c, d = 1, 3 # Adjust the values of variable \"d\" according to the required degree.\n",
    "c_A1 = c_A2 = c_A3 = c_A4 = c_A5 = c\n",
    "d_A1 = d_A2 = d_A3 = d_A4 = d_A5 = d\n",
    "solutions_A1 = [solve_for_lambda(data, c_A1, d_A1, lambda_val_A1, m) for lambda_val_A1 in lambda_values_A1]\n",
    "solutions_A2 = [solve_for_lambda_A2(data_A2, c_A2, d_A2, lambda_val_A2, m) for lambda_val_A2 in lambda_values_A2]\n",
    "solutions_A3 = [solve_for_lambda_A3(data_A3, c_A3, d_A3, lambda_val_A3, m) for lambda_val_A3 in lambda_values_A3]\n",
    "solutions_A4 = [solve_for_lambda_A4(data_A4, c_A4, d_A4, lambda_val_A4, m) for lambda_val_A4 in lambda_values_A4]\n",
    "solutions_A5 = [solve_for_lambda_A5(data_A5, c_A5, d_A5, lambda_val_A5, m) for lambda_val_A5 in lambda_values_A5]\n",
    "solutions_combined = []\n",
    "for i, (w1, w2, w3, w4, w5) in enumerate(zip(solutions_A1, solutions_A2, solutions_A3, solutions_A4, solutions_A5)):\n",
    "    added_matrix = add_matrices(w1['W1'], w2['W2'], w3['W3'], w4['W4'], w5['W5'])\n",
    "    symbols_A1 = w1['W1'].free_symbols\n",
    "    symbols_A2 = w2['W2'].free_symbols\n",
    "    symbols_A3 = w3['W3'].free_symbols\n",
    "    symbols_A4 = w4['W4'].free_symbols\n",
    "    symbols_A5 = w5['W5'].free_symbols\n",
    "    all_symbols = symbols_A1.union(symbols_A2).union(symbols_A3).union(symbols_A4).union(symbols_A5)\n",
    "    set_solution = {'y0': 1}\n",
    "    equations = [sp.Eq(added_matrix[i], 0) for i in range(added_matrix.shape[0])]\n",
    "    solutions_system = sp.solve(equations, all_symbols)\n",
    "    for var, sol in solutions_system.items():\n",
    "        set_solution[str(var)] = sol    \n",
    "    solutions_combined.append(set_solution)\n",
    "def compute_kernel_matrix(X, c, d):\n",
    "    n = X.shape[0]\n",
    "    K = (c + np.dot(X, X.T)) ** d\n",
    "    return K\n",
    "def solve_for_lambda(data, c, d, lambda_value, y_values):\n",
    "    K = compute_kernel_matrix(data, c, d)\n",
    "    I = np.eye(K.shape[0])\n",
    "    K_with_I = K + lambda_value * I\n",
    "    K_with_I_inv = np.linalg.inv(K_with_I)\n",
    "    y_repeated = np.repeat(y_values, len(data) // len(y_values))\n",
    "    alpha_sym = K_with_I_inv @ y_repeated\n",
    "    return K_with_I_inv, alpha_sym\n",
    "def generate_f_alpha_expression(alpha_sym, x1, x2, x3, x4, x_q1_sym, x_q2_sym, x_q3_sym, x_q4_sym, c, d):\n",
    "    f_alpha = 0\n",
    "    for i in range(len(alpha_sym)):\n",
    "        f_alpha += alpha_sym[i] * (c + (x1[i] * x_q1_sym) + (x2[i] * x_q2_sym) + (x3[i] * x_q3_sym) + (x4[i] * x_q4_sym)) ** d\n",
    "    f_alpha_expanded = sp.expand(f_alpha)\n",
    "    f_alpha_collected = sp.collect(f_alpha_expanded, (x_q1_sym, x_q2_sym, x_q3_sym, x_q4_sym))\n",
    "    return f_alpha_collected\n",
    "def process_dataset(file_path, c, d, lambda_values, y_values_dicts):\n",
    "    data = np.loadtxt(file_path, delimiter=',', skiprows=1, usecols=(0, 1, 2, 3))\n",
    "#     x1, x2, x3, x4 = data.T\n",
    "    x1, x2, x3, x4 = data[:, 0], data[:, 1], data[:, 2], data[:, 3]\n",
    "    K_with_I_inv_list = []\n",
    "    alpha_sym_list = []\n",
    "    f_alpha_expression_list = []\n",
    "    for lambda_val, y_values_dict in zip(lambda_values, y_values_dicts):\n",
    "        K_with_I_inv, alpha_sym = solve_for_lambda(data, c, d, lambda_val, list(y_values_dict.values()))\n",
    "        x_q1_sym, x_q2_sym, x_q3_sym, x_q4_sym= sp.symbols('x1 x2 x3 x4')\n",
    "        K_with_I_inv_list.append(K_with_I_inv)\n",
    "        alpha_sym_list.append(alpha_sym)\n",
    "        f_alpha_expression = generate_f_alpha_expression(alpha_sym, x1, x2, x3, x4, x_q1_sym, x_q2_sym, x_q3_sym, x_q4_sym, c, d)\n",
    "        f_alpha_expression_list.append(f_alpha_expression)\n",
    "    return K_with_I_inv_list, alpha_sym_list, f_alpha_expression_list\n",
    "lambda_values = [10**(-i) for i in range(lamda)]\n",
    "y_values_dicts_list = solutions_combined\n",
    "file_path_1 = \"../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B501.csv\"\n",
    "file_path_2 = \"../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B502.csv\"\n",
    "file_path_3 = \"../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B503.csv\"\n",
    "file_path_4 = \"../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B504.csv\"\n",
    "file_path_5 = \"../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B505.csv\"\n",
    "B501_data = pd.read_csv('../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B501.csv')\n",
    "df = pd.read_csv(\"../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/B505.csv\")\n",
    "traj_len = B501_data.groupby('trajectory').size()\n",
    "rep = int(round(traj_len.mean()))\n",
    "K_with_I_inv_1, alpha_sym_1, f_alpha_expression_1 = process_dataset(file_path_1, c, d, lambda_values, y_values_dicts_list)\n",
    "K_with_I_inv_2, alpha_sym_2, f_alpha_expression_2 = process_dataset(file_path_2, c, d, lambda_values, y_values_dicts_list)\n",
    "K_with_I_inv_3, alpha_sym_3, f_alpha_expression_3 = process_dataset(file_path_3, c, d, lambda_values, y_values_dicts_list)\n",
    "K_with_I_inv_4, alpha_sym_4, f_alpha_expression_4 = process_dataset(file_path_4, c, d, lambda_values, y_values_dicts_list)\n",
    "K_with_I_inv_5, alpha_sym_5, f_alpha_expression_5 = process_dataset(file_path_5, c, d, lambda_values, y_values_dicts_list)\n",
    "f_vector1_0, f_vector1_1, f_vector1_2, f_vector1_3, f_vector1_4, f_vector1_5, f_vector1_6, f_vector1_7 = [], [], [], [], [], [], [], []\n",
    "for index, row in df.iterrows():\n",
    "    value1_0, value1_1 = eval(str(f_alpha_expression_1[0]), globals(), row.to_dict()), eval(str(f_alpha_expression_1[1]), globals(), row.to_dict())\n",
    "    value1_2, value1_3 = eval(str(f_alpha_expression_1[2]), globals(), row.to_dict()), eval(str(f_alpha_expression_1[3]), globals(), row.to_dict())\n",
    "    value1_4, value1_5 = eval(str(f_alpha_expression_1[4]), globals(), row.to_dict()), eval(str(f_alpha_expression_1[5]), globals(), row.to_dict())\n",
    "    value1_6, value1_7 = eval(str(f_alpha_expression_1[6]), globals(), row.to_dict()), eval(str(f_alpha_expression_1[7]), globals(), row.to_dict())\n",
    "    f_vector1_0.append(value1_0),f_vector1_1.append(value1_1),f_vector1_2.append(value1_2),f_vector1_3.append(value1_3), f_vector1_4.append(value1_4), f_vector1_5.append(value1_5), f_vector1_6.append(value1_6), f_vector1_7.append(value1_7)\n",
    "f_vector2_0, f_vector2_1, f_vector2_2, f_vector2_3, f_vector2_4, f_vector2_5, f_vector2_6, f_vector2_7 = [], [], [], [], [], [], [], []\n",
    "for index, row in df.iterrows():\n",
    "    value2_0, value2_1 = eval(str(f_alpha_expression_2[0]), globals(), row.to_dict()), eval(str(f_alpha_expression_2[1]), globals(), row.to_dict())\n",
    "    value2_2, value2_3 = eval(str(f_alpha_expression_2[2]), globals(), row.to_dict()), eval(str(f_alpha_expression_2[3]), globals(), row.to_dict())\n",
    "    value2_4, value2_5 = eval(str(f_alpha_expression_2[4]), globals(), row.to_dict()), eval(str(f_alpha_expression_2[5]), globals(), row.to_dict())\n",
    "    value2_6, value2_7 = eval(str(f_alpha_expression_2[6]), globals(), row.to_dict()), eval(str(f_alpha_expression_2[7]), globals(), row.to_dict())\n",
    "    f_vector2_0.append(value2_0),f_vector2_1.append(value2_1),f_vector2_2.append(value2_2),f_vector2_3.append(value2_3),f_vector2_4.append(value2_4),f_vector2_5.append(value2_5),f_vector2_6.append(value2_6),f_vector2_7.append(value2_7)\n",
    "f_vector3_0, f_vector3_1, f_vector3_2, f_vector3_3, f_vector3_4, f_vector3_5, f_vector3_6, f_vector3_7 = [], [], [], [], [], [], [], []\n",
    "for index, row in df.iterrows():\n",
    "    value3_0, value3_1 = eval(str(f_alpha_expression_3[0]), globals(), row.to_dict()), eval(str(f_alpha_expression_3[1]), globals(), row.to_dict())\n",
    "    value3_2, value3_3 = eval(str(f_alpha_expression_3[2]), globals(), row.to_dict()), eval(str(f_alpha_expression_3[3]), globals(), row.to_dict())\n",
    "    value3_4, value3_5 = eval(str(f_alpha_expression_3[4]), globals(), row.to_dict()), eval(str(f_alpha_expression_3[5]), globals(), row.to_dict())\n",
    "    value3_6, value3_7 = eval(str(f_alpha_expression_3[6]), globals(), row.to_dict()), eval(str(f_alpha_expression_3[7]), globals(), row.to_dict())\n",
    "    f_vector3_0.append(value3_0),f_vector3_1.append(value3_1),f_vector3_2.append(value3_2),f_vector3_3.append(value3_3),f_vector3_4.append(value3_4),f_vector3_5.append(value3_5),f_vector3_6.append(value3_6),f_vector3_7.append(value3_7),\n",
    "f_vector4_0, f_vector4_1, f_vector4_2, f_vector4_3, f_vector4_4, f_vector4_5, f_vector4_6, f_vector4_7 = [], [], [], [], [], [], [], []\n",
    "for index, row in df.iterrows():\n",
    "    value4_0, value4_1 = eval(str(f_alpha_expression_4[0]), globals(), row.to_dict()), eval(str(f_alpha_expression_4[1]), globals(), row.to_dict())\n",
    "    value4_2, value4_3 = eval(str(f_alpha_expression_4[2]), globals(), row.to_dict()), eval(str(f_alpha_expression_4[3]), globals(), row.to_dict())\n",
    "    value4_4, value4_5 = eval(str(f_alpha_expression_4[4]), globals(), row.to_dict()), eval(str(f_alpha_expression_4[5]), globals(), row.to_dict())\n",
    "    value4_6, value4_7 = eval(str(f_alpha_expression_4[6]), globals(), row.to_dict()), eval(str(f_alpha_expression_4[7]), globals(), row.to_dict())\n",
    "    f_vector4_0.append(value4_0),f_vector4_1.append(value4_1),f_vector4_2.append(value4_2),f_vector4_3.append(value4_3),f_vector4_4.append(value4_4),f_vector4_5.append(value4_5),f_vector4_6.append(value4_6),f_vector4_7.append(value4_7)   \n",
    "f_vector5_0, f_vector5_1, f_vector5_2, f_vector5_3, f_vector5_4, f_vector5_5, f_vector5_6, f_vector5_7 = [], [], [], [], [], [], [], []\n",
    "for index, row in df.iterrows():\n",
    "    value5_0, value5_1 = eval(str(f_alpha_expression_5[0]), globals(), row.to_dict()), eval(str(f_alpha_expression_5[1]), globals(), row.to_dict())\n",
    "    value5_2, value5_3 = eval(str(f_alpha_expression_5[2]), globals(), row.to_dict()), eval(str(f_alpha_expression_5[3]), globals(), row.to_dict())\n",
    "    value5_4, value5_5 = eval(str(f_alpha_expression_5[4]), globals(), row.to_dict()), eval(str(f_alpha_expression_5[5]), globals(), row.to_dict())\n",
    "    value5_6, value5_7 = eval(str(f_alpha_expression_5[6]), globals(), row.to_dict()), eval(str(f_alpha_expression_5[7]), globals(), row.to_dict())\n",
    "    f_vector5_0.append(value5_0),f_vector5_1.append(value5_1),f_vector5_2.append(value5_2),f_vector5_3.append(value5_3),f_vector5_4.append(value5_4),f_vector5_5.append(value5_5),f_vector5_6.append(value5_6),f_vector5_7.append(value5_7)    \n",
    "h1, h2, h3, h4 = list(y_values_dicts_list[0].values()), list(y_values_dicts_list[1].values()), list(y_values_dicts_list[2].values()), list(y_values_dicts_list[3].values())\n",
    "h5, h6, h7, h8 = list(y_values_dicts_list[4].values()), list(y_values_dicts_list[5].values()), list(y_values_dicts_list[6].values()), list(y_values_dicts_list[7].values())\n",
    "y_B501_0, y_B501_1, y_B501_2, y_B501_3, y_B501_4 = np.repeat(h1, rep), np.repeat(h2, rep), np.repeat(h3, rep), np.repeat(h4, rep), np.repeat(h5, rep)\n",
    "y_B501_5, y_B501_6, y_B501_7, y_B502_0, y_B502_1 = np.repeat(h6, rep), np.repeat(h7, rep), np.repeat(h8, rep), np.repeat(h1, rep), np.repeat(h2, rep)\n",
    "y_B502_2, y_B502_3, y_B502_4, y_B502_5, y_B502_6 = np.repeat(h3, rep), np.repeat(h4, rep), np.repeat(h5, rep), np.repeat(h6, rep), np.repeat(h7, rep)\n",
    "y_B502_7, y_B503_0, y_B503_1,y_B503_2, y_B503_3 = np.repeat(h8, rep), np.repeat(h1, rep), np.repeat(h2, rep), np.repeat(h3, rep), np.repeat(h4, rep)\n",
    "y_B503_4,y_B503_5, y_B503_6, y_B503_7, y_B504_0 = np.repeat(h5, rep), np.repeat(h6, rep), np.repeat(h7, rep), np.repeat(h8, rep), np.repeat(h1, rep)\n",
    "y_B504_1, y_B504_2, y_B504_3, y_B504_4, y_B504_5 = np.repeat(h2, rep), np.repeat(h3, rep), np.repeat(h4, rep), np.repeat(h5, rep), np.repeat(h6, rep)\n",
    "y_B504_6, y_B504_7 = np.repeat(h7, rep), np.repeat(h8, rep)\n",
    "y_B505_0, y_B505_1, y_B505_2, y_B505_3, y_B505_4 = np.repeat(h1, rep), np.repeat(h2, rep), np.repeat(h3, rep), np.repeat(h4, rep), np.repeat(h5, rep)\n",
    "y_B505_5, y_B505_6, y_B505_7 = np.repeat(h6, rep), np.repeat(h7, rep), np.repeat(h8, rep)\n",
    "RMSE_1_0, RMSE_1_1 = np.sqrt(mean_squared_error(y_B501_0, f_vector1_0)), np.sqrt(mean_squared_error(y_B501_1, f_vector1_1))\n",
    "RMSE_1_2, RMSE_1_3 = np.sqrt(mean_squared_error(y_B501_2, f_vector1_2)), np.sqrt(mean_squared_error(y_B501_3, f_vector1_3))\n",
    "RMSE_1_4, RMSE_1_5 = np.sqrt(mean_squared_error(y_B501_4, f_vector1_4)), np.sqrt(mean_squared_error(y_B501_5, f_vector1_5))\n",
    "RMSE_1_6, RMSE_1_7 = np.sqrt(mean_squared_error(y_B501_6, f_vector1_6)), np.sqrt(mean_squared_error(y_B501_7, f_vector1_7))\n",
    "RMSE_2_0, RMSE_2_1 = np.sqrt(mean_squared_error(y_B502_0, f_vector2_0)), np.sqrt(mean_squared_error(y_B502_1, f_vector2_1))\n",
    "RMSE_2_2, RMSE_2_3 = np.sqrt(mean_squared_error(y_B502_2, f_vector2_2)), np.sqrt(mean_squared_error(y_B502_3, f_vector2_3))\n",
    "RMSE_2_4, RMSE_2_5 = np.sqrt(mean_squared_error(y_B502_4, f_vector2_4)), np.sqrt(mean_squared_error(y_B502_5, f_vector2_5))\n",
    "RMSE_2_6, RMSE_2_7 = np.sqrt(mean_squared_error(y_B502_6, f_vector2_6)), np.sqrt(mean_squared_error(y_B502_7, f_vector2_7))\n",
    "RMSE_3_0, RMSE_3_1 = np.sqrt(mean_squared_error(y_B503_0, f_vector3_0)), np.sqrt(mean_squared_error(y_B503_1, f_vector3_1))\n",
    "RMSE_3_2, RMSE_3_3 = np.sqrt(mean_squared_error(y_B503_2, f_vector3_2)), np.sqrt(mean_squared_error(y_B503_3, f_vector3_3))\n",
    "RMSE_3_4, RMSE_3_5 = np.sqrt(mean_squared_error(y_B503_4, f_vector3_4)), np.sqrt(mean_squared_error(y_B503_5, f_vector3_5))\n",
    "RMSE_3_6, RMSE_3_7 = np.sqrt(mean_squared_error(y_B503_6, f_vector3_6)), np.sqrt(mean_squared_error(y_B503_7, f_vector3_7))\n",
    "RMSE_4_0, RMSE_4_1 = np.sqrt(mean_squared_error(y_B504_0, f_vector4_0)), np.sqrt(mean_squared_error(y_B504_1, f_vector4_1))\n",
    "RMSE_4_2, RMSE_4_3 = np.sqrt(mean_squared_error(y_B504_2, f_vector4_2)), np.sqrt(mean_squared_error(y_B504_3, f_vector4_3))\n",
    "RMSE_4_4, RMSE_4_5 = np.sqrt(mean_squared_error(y_B504_4, f_vector4_4)), np.sqrt(mean_squared_error(y_B504_5, f_vector4_5))\n",
    "RMSE_4_6, RMSE_4_7 = np.sqrt(mean_squared_error(y_B504_6, f_vector4_6)), np.sqrt(mean_squared_error(y_B504_7, f_vector4_7))\n",
    "RMSE_5_0, RMSE_5_1 = np.sqrt(mean_squared_error(y_B505_0, f_vector5_0)), np.sqrt(mean_squared_error(y_B505_1, f_vector5_1))\n",
    "RMSE_5_2, RMSE_5_3 = np.sqrt(mean_squared_error(y_B505_2, f_vector5_2)), np.sqrt(mean_squared_error(y_B505_3, f_vector5_3))\n",
    "RMSE_5_4, RMSE_5_5 = np.sqrt(mean_squared_error(y_B505_4, f_vector5_4)), np.sqrt(mean_squared_error(y_B505_5, f_vector5_5))\n",
    "RMSE_5_6, RMSE_5_7 = np.sqrt(mean_squared_error(y_B505_6, f_vector5_6)), np.sqrt(mean_squared_error(y_B505_7, f_vector5_7))\n",
    "rmse_values = {\n",
    "    \"B501\": [RMSE_1_0, RMSE_1_1, RMSE_1_2, RMSE_1_3, RMSE_1_4, RMSE_1_5, RMSE_1_6, RMSE_1_7],\n",
    "    \"B502\": [RMSE_2_0, RMSE_2_1, RMSE_2_2, RMSE_2_3, RMSE_2_4, RMSE_2_5, RMSE_2_6, RMSE_2_7],\n",
    "    \"B503\": [RMSE_3_0, RMSE_3_1, RMSE_3_2, RMSE_3_3, RMSE_3_4, RMSE_3_5, RMSE_3_6, RMSE_3_7],\n",
    "    \"B504\": [RMSE_4_0, RMSE_4_1, RMSE_4_2, RMSE_4_3, RMSE_4_4, RMSE_4_5, RMSE_4_6, RMSE_4_7],\n",
    "    \"B505\": [RMSE_5_0, RMSE_5_1, RMSE_5_2, RMSE_5_3, RMSE_5_4, RMSE_5_5, RMSE_5_6, RMSE_5_7]\n",
    "}\n",
    "for file, rmse_list in rmse_values.items():\n",
    "    min_rmse = min(rmse_list)\n",
    "    min_index = rmse_list.index(min_rmse)\n",
    "rmse_values = [RMSE_1_0, RMSE_1_1, RMSE_1_2, RMSE_1_3, RMSE_1_4, RMSE_1_5, RMSE_1_6, RMSE_1_7,\n",
    "               RMSE_2_0, RMSE_2_1, RMSE_2_2, RMSE_2_3, RMSE_2_4, RMSE_2_5, RMSE_2_6, RMSE_2_7,\n",
    "               RMSE_3_0, RMSE_3_1, RMSE_3_2, RMSE_3_3, RMSE_3_4, RMSE_3_5, RMSE_3_6, RMSE_3_7,\n",
    "               RMSE_4_0, RMSE_4_1, RMSE_4_2, RMSE_4_3, RMSE_4_4, RMSE_4_5, RMSE_4_6, RMSE_4_7,\n",
    "               RMSE_5_0, RMSE_5_1, RMSE_5_2, RMSE_5_3, RMSE_5_4, RMSE_5_5, RMSE_5_6, RMSE_5_7]\n",
    "min_rmse = min(rmse_values)\n",
    "min_index = rmse_values.index(min_rmse)\n",
    "file_index = min_index // 8 + 1\n",
    "sub_index = min_index % 8\n",
    "h_values = [h1, h2, h3, h4, h5, h6, h7, h8]\n",
    "lambda_values = [10**(-i) for i in range(lamda)]\n",
    "h_value = h_values[sub_index]\n",
    "df2 = pd.read_csv('../../../System/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/50.csv')\n",
    "df2['trajectory'] = df2['trajectory'].replace({i: h_value[i-1] for i in range(1, m+1)})\n",
    "# df2['trajectory'] = df2['trajectory'].replace({1: h_value[0],2: h_value[1], 3: h_value[2], 4: h_value[3], 5: h_value[4]})\n",
    "X = df2.iloc[:, :-1]\n",
    "y = df2.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train.to_csv('../../../results/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/X_train.csv', index=False)\n",
    "y_train = y_train.astype(float)\n",
    "def polynomial_kernel(X, Y, degree=3):\n",
    "    return (1 + np.dot(X, Y.T)) ** degree\n",
    "param_grid = {'alpha': [0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kr_model = KernelRidge(kernel=polynomial_kernel)\n",
    "grid_search = GridSearchCV(kr_model, param_grid, cv=cv, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best RMSE:\", -grid_search.best_score_)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "grid_search.best_estimator_\n",
    "y_pred = grid_search.predict(X_test)\n",
    "class KernelMethodBase(object):\n",
    "    '''\n",
    "    Base class for kernel methods models\n",
    "    Methods\n",
    "    ----\n",
    "    fit\n",
    "    predict\n",
    "    fit_K\n",
    "    predict_K\n",
    "    '''\n",
    "    kernels_ = {\n",
    "        'polynomial': polynomial_kernel,\n",
    "    }\n",
    "    def __init__(self, kernel='polynomial', **kwargs):\n",
    "        self.kernel_name = kernel\n",
    "        self.kernel_function_ = self.kernels_[kernel]\n",
    "        self.kernel_parameters = self.get_kernel_parameters(**kwargs)\n",
    "        self.fit_intercept_ = False\n",
    "    def get_kernel_parameters(self, **kwargs):\n",
    "        params = {}\n",
    "        params['degree'] = kwargs.get('degree', 3)\n",
    "        return params\n",
    "    def fit_K(self, K, y, **kwargs):\n",
    "        pass\n",
    "    def decision_function_K(self, K):\n",
    "        pass\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        K = self.kernel_function_(self.X_train, self.X_train, **self.kernel_parameters)\n",
    "        return self.fit_K(K, y, **kwargs)\n",
    "    def decision_function(self, X):\n",
    "        K_x = self.kernel_function_(X, self.X_train, **self.kernel_parameters)\n",
    "        return self.decision_function_K(K_x)\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    def predict_K(self, K):\n",
    "        pass\n",
    "class KernelRidgeRegression(KernelMethodBase):\n",
    "    '''\n",
    "    Kernel Ridge Regression\n",
    "    '''\n",
    "    def __init__(self, alpha=0.1, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(KernelRidgeRegression, self).__init__(**kwargs)\n",
    "    def fit_K(self, K, y):\n",
    "        n = K.shape[0]\n",
    "        assert (n == len(y))\n",
    "        A = K + self.alpha*np.identity(n)\n",
    "        self.eta = np.linalg.solve(A , y)\n",
    "        return self\n",
    "    def decision_function_K(self, K_x):\n",
    "        return K_x.dot(self.eta)\n",
    "    def predict(self, X):\n",
    "        return self.decision_function(X)\n",
    "    def predict_K(self, K_x):\n",
    "        return self.decision_function_K(K_x)\n",
    "kernel = 'polynomial'\n",
    "kr_model = KernelRidgeRegression(\n",
    "    kernel=kernel,\n",
    "    alpha=grid_search.best_params_['alpha'],\n",
    "    )\n",
    "kr_model.fit(X_train, y_train)\n",
    "eta = kr_model.eta\n",
    "x1, x2, x3, x4 = sp.symbols('x1 x2 x3 x4') # all variable\n",
    "polynomial_kernel = (1 + x1*sp.Symbol('xi1') + x2*sp.Symbol('xi2') + x3*sp.Symbol('xi3') + x4*sp.Symbol('xi4'))**3\n",
    "f_beta = 0\n",
    "for i in range(len(X_train)):\n",
    "    f_beta += eta[i] * polynomial_kernel.subs({'xi1': X_train.iloc[i][0], 'xi2': X_train.iloc[i][1], 'xi3': X_train.iloc[i][2], 'xi4': X_train.iloc[i][3]})\n",
    "candidate_CL = sp.expand(f_beta)\n",
    "print(\"Candidate Conservation Law:\")\n",
    "sp.pprint(candidate_CL)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "with open(\"../../../results/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/candidate_CL.txt\", \"w\") as file:\n",
    "    file.write(str(candidate_CL))\n",
    "# print(\"Final Candidate CL:\")\n",
    "# sp.pprint(filtered_expression)\n",
    "# print(\"\")\n",
    "# print(\"\")\n",
    "coef = list(candidate_CL.as_coefficients_dict().values())\n",
    "terms = list(candidate_CL.as_coefficients_dict().keys())\n",
    "filtered_terms = [term for coeff, term in zip(coef, terms) if abs(coeff) > 0.0001] # remove all terms which is less that 0.0001\n",
    "filtered = sum(sp.Mul(coeff, term) for coeff, term in zip(coef, terms) if term in filtered_terms)\n",
    "print(\"Final Candidate CL:\")\n",
    "sp.pprint(filtered)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "exp = filtered\n",
    "tar = x1**2 * x2\n",
    "tar_coef = exp.coeff(tar)\n",
    "deno = tar_coef\n",
    "div = exp / deno\n",
    "simp_exp = simplify(div)\n",
    "# print(\"Simplified Candidate CL:\")\n",
    "# print(simp_exp)\n",
    "# print(\"\")\n",
    "# print(\"\")\n",
    "filtered_exp = sum(term for term in simp_exp.args if term.has(x1) or term.has(x2) or term.has(x3) or term.has(x4))\n",
    "print(\"Simplified Candidate CL:\")\n",
    "sp.pprint(filtered_exp)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "with open(\"../../../results/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/candidate_CL.txt\", \"w\") as file:\n",
    "    file.write(str(candidate_CL))\n",
    "with open(\"../../../results/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/candidate_CL.txt\", \"r\") as file:\n",
    "    candidate_CL = sp.sympify(file.read())\n",
    "df3 = pd.read_csv('../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/holdoutp_data50.csv')\n",
    "traj_len = df3.groupby('trajectory').size()\n",
    "rep1 = int(round(traj_len.mean()))\n",
    "expression = sp.lambdify((x1, x2, x3, x4), candidate_CL, \"numpy\")\n",
    "df3['lamhold'] = expression(df3['x1'], df3['x2'], df3['x3'], df3['x4'])\n",
    "da = {'y{}'.format(i): h_value[i] for i in range(len(h_value))}\n",
    "# da = {'y0': h_value[0], 'y1': h_value[1], 'y2': h_value[2], 'y3': h_value[3], 'y4': h_value[4]}\n",
    "df3['Coluh(lamhold)'] = [da[f'y{i}'] for i in range(m) for _ in range(rep1)]\n",
    "columns_to_compare = [('lamhold', 'Coluh(lamhold)')]\n",
    "for col1, col2 in columns_to_compare:\n",
    "    rmse = np.sqrt(mean_squared_error(df3[col1], df3[col2]))\n",
    "    print(f'Generalisation Error (RMSE): {rmse}')\n",
    "    print(\"\")\n",
    "with open(\"../../../results/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/candidate_CL.txt\", \"r\") as file:\n",
    "    total_sum = sp.sympify(file.read())\n",
    "f = sp.lambdify((x1, x2, x3, x4), candidate_CL, \"numpy\")\n",
    "dat = pd.read_csv('../../../System/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/50.csv')\n",
    "trajectories = dat['trajectory'].unique()\n",
    "total_sum_squared_normalized_functional_value = 0\n",
    "total_data_points = 0\n",
    "for trajectory in trajectories:\n",
    "    trajectory_data = dat[dat['trajectory'] == trajectory].copy()  \n",
    "    cols = ['x' + str(i) for i in range(1, num_x_variables + 1)] # number of variable\n",
    "    trajectory_data['functional_value'] = f(*trajectory_data[cols].values.T)\n",
    "    mean_value = trajectory_data['functional_value'].mean()\n",
    "    trajectory_data['functional_value_minus_mean'] = trajectory_data['functional_value'] - mean_value\n",
    "    trajectory_data['normalized_functional_value'] = trajectory_data['functional_value_minus_mean'] / mean_value\n",
    "    trajectory_data['squared_normalized_functional_value'] = trajectory_data['normalized_functional_value'] ** 2\n",
    "    total_sum_squared_normalized_functional_value += trajectory_data['squared_normalized_functional_value'].sum()\n",
    "    total_data_points += len(trajectory_data)\n",
    "average_squared_normalized_functional_value = total_sum_squared_normalized_functional_value / total_data_points\n",
    "standard_deviation = math.sqrt(average_squared_normalized_functional_value)\n",
    "print(\" Relative deviation:\", standard_deviation)\n",
    "print(\"\")\n",
    "input_directory = r'../../../System/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60'\n",
    "output_directory = r'../../../Data/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/d'\n",
    "df14 = pd.read_csv(os.path.join(input_directory, '50.csv')) # 50.csv, saved names of the data\n",
    "Br = pd.read_csv('../../../System/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/50.csv')\n",
    "tr = Br.groupby('trajectory').size()\n",
    "re1 = int(round(tr.mean()))\n",
    "rows_per_file = re1 \n",
    "num_files = len(df14) // rows_per_file\n",
    "data_chunks = [df14.iloc[i * rows_per_file:(i + 1) * rows_per_file] for i in range(num_files)]\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "for i, chunk in enumerate(data_chunks):\n",
    "    chunk.to_csv(os.path.join(output_directory, f'q{i + 1}.csv'), index=False)\n",
    "expression = total_sum\n",
    "with open('../../../System/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/50.csv', 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader) \n",
    "    current_trajectory = None\n",
    "    expression_values = []\n",
    "    for row in csv_reader:\n",
    "        x1_val, x2_val, x3_val, x4_val, trajectory = map(float, row)\n",
    "        if trajectory != current_trajectory:\n",
    "            current_trajectory = trajectory\n",
    "            initial_data = [x1_val, x2_val, x3_val, x4_val]\n",
    "            expression_value = expression.subs({x1: x1_val, x2: x2_val, x3: x3_val, x4: x4_val})\n",
    "            expression_values.append(expression_value)\n",
    "compute_function = sp.lambdify((x1, x2, x3, x4), candidate_CL, 'numpy')\n",
    "for i in range(1, m+1):\n",
    "    input_file = os.path.join(output_directory, f'q{i}.csv')\n",
    "    output_file = os.path.join(output_directory, f'r{i}.csv')\n",
    "    data = pd.read_csv(input_file)\n",
    "    data['computed_value'] = compute_function(data['x1'], data['x2'], data['x3'], data['x4'])\n",
    "    data.to_csv(output_file, index=False)\n",
    "subtraction_values = expression_values\n",
    "for i in range(1, m+1): # 5 trajectory\n",
    "    r_file = os.path.join(output_directory, f'r{i}.csv')\n",
    "    n_file = os.path.join(output_directory, f'n{i}.csv')\n",
    "    subtraction_value = subtraction_values[i - 1]\n",
    "    data = pd.read_csv(r_file)\n",
    "    data['adjusted_value'] = data['computed_value'] - subtraction_value\n",
    "    data.to_csv(n_file, columns=['adjusted_value'], index=False)\n",
    "all_data = pd.DataFrame()\n",
    "subtraction_values = expression_values\n",
    "for i in range(1, m+1): # 5 trajectory\n",
    "    r_file = os.path.join(output_directory, f'r{i}.csv')\n",
    "    n_file = os.path.join(output_directory, f'n{i}.csv')\n",
    "    subtraction_value = subtraction_values[i - 1]\n",
    "    data = pd.read_csv(r_file)\n",
    "    data['adjusted_value'] = (data['computed_value'] - subtraction_value)/subtraction_value\n",
    "    data.to_csv(n_file, columns=['adjusted_value'], index=False)\n",
    "    all_data[f'n{i}'] = data['adjusted_value']\n",
    "plt.figure(figsize=(10, 6))\n",
    "for column in all_data.columns:\n",
    "    plt.plot(all_data[column], label=column)\n",
    "plt.xlabel('N', fontsize=16)\n",
    "plt.ylabel('Relative Variation', fontsize=16)\n",
    "save_path = '../../../results/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/Relative_Variation.png'\n",
    "plt.savefig(save_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "329d1cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Expression:\n",
      "      2                          2                       3                    \n",
      "1.0⋅x₁ ⋅x₂ + 0.500002797515942⋅x₁  - 0.333331811732937⋅x₂  + 0.500005737362502\n",
      "\n",
      "   2                       2                       2\n",
      "⋅x₂  + 0.500006840245901⋅x₃  + 0.500006707393184⋅x₄ \n",
      "\n",
      "\n",
      "Perturbed Expression:\n",
      "     2        3        2        2        2         2                          \n",
      "a₁⋅x₁  + a₂⋅x₂  + a₃⋅x₂  + a₄⋅x₃  + a₅⋅x₄  + 1.0⋅x₁ ⋅x₂ + 0.500002797515942⋅x₁\n",
      "\n",
      "2                       3                       2                       2     \n",
      "  - 0.333331811732937⋅x₂  + 0.500005737362502⋅x₂  + 0.500006840245901⋅x₃  + 0.\n",
      "\n",
      "                  2\n",
      "500006707393184⋅x₄ \n",
      "Expanded Equation:\n",
      "                    2                                                       2 \n",
      "2⋅a₁⋅x₁⋅x₃ + 3⋅a₂⋅x₂ ⋅x₄ + 2⋅a₃⋅x₂⋅x₄ - 4⋅a₄⋅x₁⋅x₂⋅x₃ - 2⋅a₄⋅x₁⋅x₃ - 2⋅a₅⋅x₁ ⋅\n",
      "\n",
      "            2                                         2                       \n",
      "x₄ + 2⋅a₅⋅x₂ ⋅x₄ - 2⋅a₅⋅x₂⋅x₄ - 1.34147863681822e-5⋅x₁ ⋅x₄ - 2.73609836041189e\n",
      "\n",
      "                                                                2             \n",
      "-5⋅x₁⋅x₂⋅x₃ - 8.08545991870524e-6⋅x₁⋅x₃ + 1.79795875557831e-5⋅x₂ ⋅x₄ - 1.94006\n",
      "\n",
      "                  \n",
      "136466918e-6⋅x₂⋅x₄\n",
      "\n",
      "\n",
      "Perturbation a1, a2, a3, a4, a5:\n",
      "\n",
      "{a₁: -2.79751594167711e-6, a₂: -1.52160039586697e-6, a₃: -5.73736250175651e-6,\n",
      " a₄: -6.84024590102972e-6, a₅: -6.7073931840911e-6}\n",
      "\n",
      "\n",
      "Final (refined) conservation law:\n",
      "      2            2                       3         2         2         2\n",
      "1.0⋅x₁ ⋅x₂ + 0.5⋅x₁  - 0.333333333333333⋅x₂  + 0.5⋅x₂  + 0.5⋅x₃  + 0.5⋅x₄ \n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, Function, diff, Eq, solve, simplify, Add\n",
    "from sympy import collect\n",
    "import sympy as sp\n",
    "a1, a2, a3, a4, a5 = symbols('a1 a2 a3 a4 a5')\n",
    "perturbed_exp = filtered_exp + a1*x1**2 + a2*x2**3 + a3*x2**2 + a4*x3**2 + a5*x4**2\n",
    "def add_perturbation(expr, a1, a2, a3, a4, a5):\n",
    "    x1, x2, x3, x4 = sp.symbols('x1 x2 x3 x4')\n",
    "    new_expr = perturbed_exp\n",
    "    return new_expr\n",
    "x1, x2, x3, x4 = sp.symbols('x1 x2 x3 x4')\n",
    "a1, a2, a3, a4, a5 = sp.symbols('a1 a2 a3 a4 a5')\n",
    "original_expr = filtered_exp\n",
    "perturbed_expr = add_perturbation(original_expr, a1, a2, a3, a4, a5)\n",
    "print(\"Original Expression:\")\n",
    "sp.pprint(original_expr)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Perturbed Expression:\")\n",
    "sp.pprint(perturbed_expr)\n",
    "Phi = perturbed_expr\n",
    "f1 = x3\n",
    "f2 = x4\n",
    "f3 = -x1-2*x1*x2\n",
    "f4 = -x2-x1**2+x2**2\n",
    "equation = f1 * sp.diff(Phi, x1) + f2 * sp.diff(Phi, x2) + f3 * sp.diff(Phi, x3) + f4 * sp.diff(Phi, x4)\n",
    "equation_simplified = equation.simplify()\n",
    "# print(\"Simplified Equation:\")\n",
    "# sp.pprint(equation_simplified)\n",
    "expanded_equation = equation_simplified.expand()\n",
    "print(\"Expanded Equation:\")\n",
    "sp.pprint(expanded_equation)\n",
    "rewritten_equation = collect(expanded_equation, [x1*x3,x2**2*x4,x2*x4,x1*x2*x3,x1**2*x4])\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "coeff_b_x1_x2 = rewritten_equation.coeff(x1*x3)\n",
    "coeff_x1_x2_x3 = rewritten_equation.coeff(x2**2*x4)\n",
    "coeff3 = rewritten_equation.coeff(x2*x4)\n",
    "coeff4 = rewritten_equation.coeff(x1*x2*x3)\n",
    "coeff5 = rewritten_equation.coeff(x1**2*x4)\n",
    "eq1 = Eq(coeff_b_x1_x2, 0)\n",
    "eq2 = Eq(coeff_x1_x2_x3, 0)\n",
    "eq3 = Eq(coeff3, 0)\n",
    "eq4 = Eq(coeff4, 0)\n",
    "eq5 = Eq(coeff5, 0)\n",
    "solution = solve((eq1, eq2, eq3, eq4, eq5), (a1, a2, a3, a4, a5))\n",
    "print(\"Perturbation a1, a2, a3, a4, a5:\")\n",
    "print(\"\")\n",
    "sp.pprint(solution)\n",
    "solution_a1 = solution[a1]\n",
    "solution_a2 = solution[a2]\n",
    "solution_a3 = solution[a3]\n",
    "solution_a4 = solution[a4]\n",
    "solution_a5 = solution[a5]\n",
    "Phi1 = perturbed_exp\n",
    "Phi1_substituted = Phi1.subs({a1: solution_a1, a2: solution_a2, a3: solution_a3, a4: solution_a4, a5: solution_a5})\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Final (refined) conservation law:\")\n",
    "sp.pprint(Phi1_substituted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cec9f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.35647208644994e-8*x1**3 + 0.0100544965859799*x1**2*x2 + 1.81783861864276e-7*x1**2*x3 - 1.8796218007676e-9*x1**2*x4 + 0.00502727642060443*x1**2 + 5.1409054928449e-8*x1*x2**2 + 5.68658355332759e-9*x1*x2*x3 + 1.82451909983168e-7*x1*x2*x4 + 1.65587007112727e-8*x1*x2 + 1.10702047571387e-7*x1*x3**2 + 1.78906790477678e-7*x1*x3*x4 + 6.69243406786063e-10*x1*x3 + 2.22066017379225e-8*x1*x4**2 + 8.71497757759434e-8*x1*x4 - 1.1734603924847e-8*x1 - 0.00335148356306731*x2**3 + 2.38681120844791e-7*x2**2*x3 - 4.78447435093904e-8*x2**2*x4 + 0.00502730597928164*x2**2 - 2.5361823652591e-7*x2*x3**2 + 4.41246110672753e-8*x2*x3*x4 - 9.30704700911889e-8*x2*x3 + 6.56799490119102e-8*x2*x4**2 + 7.38752224324624e-10*x2*x4 - 2.21011931585829e-9*x2 + 2.32967860800234e-7*x3**3 + 3.45116822309758e-8*x3**2*x4 + 0.00502731706821901*x3**2 + 3.57896136793227e-7*x3*x4**2 + 1.67725370866373e-8*x3*x4 - 2.77420470224526e-8*x3 - 7.02108648132044e-8*x4**3 + 0.00502731573245182*x4**2 + 7.69804035620492e-9*x4 + 0.999466228406152\n"
     ]
    }
   ],
   "source": [
    "print(candidate_CL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30a709f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression \n",
    "dataa = pd.read_csv('../../../System/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/50.csv')\n",
    "def f1(x1, x2, x3, x4):\n",
    "    return 9.35647208644994e-8*x1**3 + 0.0100544965859799*x1**2*x2 + 1.81783861864276e-7*x1**2*x3 - 1.8796218007676e-9*x1**2*x4 + 0.00502727642060443*x1**2 + 5.1409054928449e-8*x1*x2**2 + 5.68658355332759e-9*x1*x2*x3 + 1.82451909983168e-7*x1*x2*x4 + 1.65587007112727e-8*x1*x2 + 1.10702047571387e-7*x1*x3**2 + 1.78906790477678e-7*x1*x3*x4 + 6.69243406786063e-10*x1*x3 + 2.22066017379225e-8*x1*x4**2 + 8.71497757759434e-8*x1*x4 - 1.1734603924847e-8*x1 - 0.00335148356306731*x2**3 + 2.38681120844791e-7*x2**2*x3 - 4.78447435093904e-8*x2**2*x4 + 0.00502730597928164*x2**2 - 2.5361823652591e-7*x2*x3**2 + 4.41246110672753e-8*x2*x3*x4 - 9.30704700911889e-8*x2*x3 + 6.56799490119102e-8*x2*x4**2 + 7.38752224324624e-10*x2*x4 - 2.21011931585829e-9*x2 + 2.32967860800234e-7*x3**3 + 3.45116822309758e-8*x3**2*x4 + 0.00502731706821901*x3**2 + 3.57896136793227e-7*x3*x4**2 + 1.67725370866373e-8*x3*x4 - 2.77420470224526e-8*x3 - 7.02108648132044e-8*x4**3 + 0.00502731573245182*x4**2 + 7.69804035620492e-9*x4 + 0.999466228406152\n",
    "def f2(x1, x2, x3, x4):\n",
    "    return 1/2*(x1**2+x2**2+x3**2+x4**2)-1/3*x2**3+x1**2*x2\n",
    "result_f1 = dataa.apply(lambda row: f1(row['x1'], row['x2'], row['x3'], row['x4']), axis=1)\n",
    "result_f2 = dataa.apply(lambda row: f2(row['x1'], row['x2'], row['x3'], row['x4']), axis=1)\n",
    "trajectories = dataa['trajectory']\n",
    "plt.figure(figsize=(4, 3))\n",
    "correlation = np.corrcoef(result_f1, result_f2)[-1, 1]\n",
    "colormap = plt.cm.get_cmap('viridis', max(trajectories) + 1)\n",
    "plt.scatter(result_f2, result_f1, c=trajectories, cmap=colormap,  s=100)\n",
    "model = LinearRegression()\n",
    "model.fit(result_f2.values.reshape(-1, 1), result_f1.values)\n",
    "plt.plot(result_f2, model.predict(result_f2.values.reshape(-1, 1)), color='blue')\n",
    "# approximation = \"≈\"\n",
    "# text_str = \"$(M, N)=(5, 30)$\\n$R^2 = 0.8$\".format(approximation)\n",
    "# plt.text(result_f2.max(), result_f1.min(), text_str, ha='right', va='bottom', color='red', fontsize=16)\n",
    "# Remove x-axis and y-axis ticks\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Exact CL', fontsize=16)\n",
    "plt.ylabel('Learned CL', fontsize=16)\n",
    "plt.tight_layout()\n",
    "save_path = '../../../results/Henon_non_chaotic_T5_40_50_60_70_80_90_100/Henon_non_chaotic_T5_60/Correlation.png'\n",
    "plt.savefig(save_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebfd02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
