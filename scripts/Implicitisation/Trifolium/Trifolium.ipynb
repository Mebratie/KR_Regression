{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca23ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter degree for polynomial kernel: 4\n",
      "Best parameters: {'alpha': 1e-07}\n",
      "Best RMSE: 0.001250000000000095\n",
      "\n",
      "\n",
      "Candidate Conservation Law Phi(x):\n",
      "                    4                         3                          3    \n",
      "0.499999872542063⋅x₁  - 9.54461761102504e-9⋅x₁ ⋅x₂ - 0.500000022055622⋅x₁  + 0\n",
      "\n",
      "                   2   2                         2                            \n",
      ".999998298548631⋅x₁ ⋅x₂  + 4.51837462583926e-9⋅x₁ ⋅x₂ + 8.98960456630071e-8⋅x₁\n",
      "\n",
      "2                             3                         2                     \n",
      "  - 3.52005041000941e-10⋅x₁⋅x₂  + 1.49999862778012⋅x₁⋅x₂  + 3.77999222200831e-\n",
      "\n",
      "                                                       4                      \n",
      "9⋅x₁⋅x₂ + 7.34290120651959e-8⋅x₁ + 0.499999742324618⋅x₂  + 4.99871877831376e-1\n",
      "\n",
      "    3                         2                                               \n",
      "0⋅x₂  - 4.95600721642231e-8⋅x₂  - 3.96436397598343e-10⋅x₂ - 2.73127875871637e-\n",
      "\n",
      " \n",
      "9\n",
      "\n",
      "\n",
      "Final Candidate CL:\n",
      "                    4                       3                       2   2     \n",
      "0.499999872542063⋅x₁  - 0.500000022055622⋅x₁  + 0.999998298548631⋅x₁ ⋅x₂  + 1.\n",
      "\n",
      "                    2                       4\n",
      "49999862778012⋅x₁⋅x₂  + 0.499999742324618⋅x₂ \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "import csv\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sympy import symbols, simplify, lambdify, Function, diff\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "%matplotlib inline\n",
    "dm = pd.read_csv(\"../../../System/Implicitisation/Trifolium/50.csv\")\n",
    "x_columns = [col for col in dm.columns if col.startswith('x')]\n",
    "num_x_variables = len(x_columns)\n",
    "d = int(input(\"Enter degree for polynomial kernel: \"))\n",
    "df2 = pd.read_csv('../../../System/Implicitisation/Trifolium/50.csv')\n",
    "X = df2.iloc[:, :-1]\n",
    "y = df2.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.to_csv('../../../results/Implicitisation/Trifolium/X_train.csv', index=False)\n",
    "y_train = y_train.astype(float)\n",
    "def polynomial_kernel(X, Y, degree=d):\n",
    "    return (1 + np.dot(X, Y.T)) ** degree\n",
    "param_grid = {'alpha': [0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1]}\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kr_model = KernelRidge(kernel=polynomial_kernel)\n",
    "grid_search = GridSearchCV(kr_model, param_grid, cv=cv, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best RMSE:\", -grid_search.best_score_)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "grid_search.best_estimator_\n",
    "y_pred = grid_search.predict(X_test)\n",
    "class KernelMethodBase(object):\n",
    "    '''\n",
    "    Base class for kernel methods models\n",
    "    Methods\n",
    "    ----\n",
    "    fit\n",
    "    predict\n",
    "    fit_K\n",
    "    predict_K\n",
    "    '''\n",
    "    kernels_ = {\n",
    "        'polynomial': polynomial_kernel,\n",
    "    }\n",
    "    def __init__(self, kernel='polynomial', **kwargs):\n",
    "        self.kernel_name = kernel\n",
    "        self.kernel_function_ = self.kernels_[kernel]\n",
    "        self.kernel_parameters = self.get_kernel_parameters(**kwargs)\n",
    "        self.fit_intercept_ = False\n",
    "    def get_kernel_parameters(self, **kwargs):\n",
    "        params = {}\n",
    "        params['degree'] = kwargs.get('degree', d)\n",
    "        return params\n",
    "    def fit_K(self, K, y, **kwargs):\n",
    "        pass\n",
    "    def decision_function_K(self, K):\n",
    "        pass\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        K = self.kernel_function_(self.X_train, self.X_train, **self.kernel_parameters)\n",
    "        return self.fit_K(K, y, **kwargs)\n",
    "    def decision_function(self, X):\n",
    "        K_x = self.kernel_function_(X, self.X_train, **self.kernel_parameters)\n",
    "        return self.decision_function_K(K_x)\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    def predict_K(self, K):\n",
    "        pass\n",
    "class KernelRidgeRegression(KernelMethodBase):\n",
    "    '''\n",
    "    Kernel Ridge Regression\n",
    "    '''\n",
    "    def __init__(self, alpha=0.1, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(KernelRidgeRegression, self).__init__(**kwargs)\n",
    "    def fit_K(self, K, y):\n",
    "        n = K.shape[0]\n",
    "        assert (n == len(y))\n",
    "        A = K + self.alpha*np.identity(n)\n",
    "        self.eta = np.linalg.solve(A , y)\n",
    "        return self\n",
    "    def decision_function_K(self, K_x):\n",
    "        return K_x.dot(self.eta)\n",
    "    def predict(self, X):\n",
    "        return self.decision_function(X)\n",
    "    def predict_K(self, K_x):\n",
    "        return self.decision_function_K(K_x)\n",
    "kernel = 'polynomial'\n",
    "kr_model = KernelRidgeRegression(\n",
    "    kernel=kernel,\n",
    "    alpha=grid_search.best_params_['alpha'],\n",
    "    )\n",
    "kr_model.fit(X_train, y_train)\n",
    "eta = kr_model.eta\n",
    "x1, x2 = sp.symbols('x1 x2')\n",
    "polynomial_kernel = (1 + x1*sp.Symbol('xi1') + x2*sp.Symbol('xi2'))**d\n",
    "f_beta = 0\n",
    "for i in range(len(X_train)):\n",
    "    f_beta += eta[i] * polynomial_kernel.subs({'xi1': X_train.iloc[i][0], 'xi2': X_train.iloc[i][1]})\n",
    "expanded_f_beta = sp.expand(f_beta)\n",
    "print(\"Candidate Conservation Law Phi(x):\")\n",
    "sp.pprint(expanded_f_beta)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "coef = list(expanded_f_beta.as_coefficients_dict().values())\n",
    "terms = list(expanded_f_beta.as_coefficients_dict().keys())\n",
    "filtered_terms = [term for coeff, term in zip(coef, terms) if abs(coeff) > 0.0001]\n",
    "filtered = sum(sp.Mul(coeff, term) for coeff, term in zip(coef, terms) if term in filtered_terms)\n",
    "print(\"Final Candidate CL:\")\n",
    "sp.pprint(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = sp.symbols('x1 x2')\n",
    "def multilinear_coefficient(n, *ks):\n",
    "    numerator = math.factorial(n)\n",
    "    denominator = 1\n",
    "    for k in ks:\n",
    "        denominator *= math.factorial(k)\n",
    "    return numerator // denominator\n",
    "def multilinear_expansion(variables, n, row):\n",
    "    expansions = []\n",
    "    for ks in itertools.product(range(n + 1), repeat=len(variables)):\n",
    "        if sum(ks) == n:\n",
    "            coefficient = multilinear_coefficient(n, *ks)\n",
    "            values = [row[var] ** k if var != '1' else 1 for var, k in zip(variables, ks)]\n",
    "            term_value = coefficient * math.prod(values)\n",
    "            expansions.append(term_value)\n",
    "    return expansions[::-1]\n",
    "def multilinear_expansion1(variables1, n1):\n",
    "    expansions1 = []  \n",
    "    for ks in itertools.product(range(n1 + 1), repeat=len(variables1)):\n",
    "        if sum(ks) == n1:\n",
    "            terms1 = [f\"{var}**{k}\" if k != 0 else f\"{var}\" for var, k in zip(variables1, ks) if k != 0]\n",
    "            term1 = \" * \".join(terms1)\n",
    "            expansions1.append(term1)\n",
    "    return expansions1[::-1]\n",
    "def generate_inner_products(coefficients, terms):\n",
    "    inner_products = [f\"{c}*{t}\" for c, t in zip(coefficients, terms)]\n",
    "    return inner_products\n",
    "num_variables = 2\n",
    "data = pd.read_csv('C:\\\\Users\\\\mebratie\\\\Desktop\\\\KR\\\\KR_Regression\\\\results\\\\Implicitisation\\\\Trifolium\\\\X_train.csv')\n",
    "variables = ['c'] + [f'x{i}' for i in range(1, num_variables + 1)]\n",
    "n = 4\n",
    "data['c'] = 1\n",
    "variables1 = ['1'] + [f'x{i}' for i in range(1, num_variables + 1)]\n",
    "n1 = 4\n",
    "all_entries = []\n",
    "for index, row in data.iterrows():\n",
    "    result = multilinear_expansion(variables, n, row)\n",
    "    result1 = multilinear_expansion1(variables1, n1)\n",
    "    expressions = generate_inner_products(result, result1)\n",
    "    all_entries.append(expressions)\n",
    "total_sum = 0\n",
    "for entry_index, (entry, alpha) in enumerate(zip(all_entries, eta), 1):\n",
    "    entry_sum = 0\n",
    "    for term in entry:\n",
    "        result = alpha * sp.sympify(term)\n",
    "        entry_sum += result\n",
    "    total_sum += entry_sum\n",
    "coefficients = list(total_sum.as_coefficients_dict().values())\n",
    "terms = list(total_sum.as_coefficients_dict().keys())\n",
    "filtered_terms = [term for coeff, term in zip(coefficients, terms) if abs(coeff) > 0.1]\n",
    "filtered_expression = sum(sp.Mul(coeff, term) for coeff, term in zip(coefficients, terms) if term in filtered_terms)\n",
    "print(\"Candidate Conservation Law:\")\n",
    "sp.pprint(total_sum)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "with open(\"C:\\\\Users\\\\mebratie\\\\Desktop\\\\KR\\\\KR_Regression\\\\results\\\\Implicitisation\\\\Trifolium\\\\total_sum.txt\", \"w\") as file:\n",
    "    file.write(str(total_sum))\n",
    "print(\"Final Candidate CL:\")\n",
    "sp.pprint(filtered_expression)\n",
    "print(\"\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beec079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
